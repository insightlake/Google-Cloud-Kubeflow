{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Docker Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ml-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login -u <docker-username> -p <password>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.py\n",
    "gs_bucket_name=\"data-labeling-demo\"\n",
    "Bucket_uri=\"gs://data-labeling-demo\"\n",
    "version=1\n",
    "#it will create fresh artifact version after each run\n",
    "store_artifacts=Bucket_uri + \"/\" + str(version)\n",
    "data_path=Bucket_uri + \"/\" + \"data/data_raw.csv\"\n",
    "processed_data=Bucket_uri + \"/\" + \"processed/data_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tensorflow/tensorflow:2.2.0-gpu\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "# Install apt dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    git \\\n",
    "    gpg-agent \\\n",
    "    python3-cairocffi \\\n",
    "    protobuf-compiler \\\n",
    "    python3-pil \\\n",
    "    python3-lxml \\\n",
    "    python3-tk \\\n",
    "    wget\n",
    "# Install gcloud and gsutil commands\n",
    "# https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu\n",
    "RUN export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\" && \\\n",
    "    echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
    "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \\\n",
    "    apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "WORKDIR /pipeline\n",
    "COPY ./ ./\n",
    "RUN pip install -r requirements.txt\n",
    "RUN pip install \"dask[dataframe]\" --upgrade\n",
    "ENV TF_CPP_MIN_LOG_LEVEL 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t <docker-username>/kubeflow-sdk-ml-training ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Docker Image to Docker hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push <docker-username>/kubeflow-sdk-ml-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Kubeflow SDk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define kubeflow pipeline Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.component\n",
    "def get_data():\n",
    "    # Defining component configuration\n",
    "    training_component = kfp.dsl.ContainerOp(\n",
    "        name='Data-Preparation',\n",
    "        image='docker.io/<docker_username>/kubeflow-sdk',\n",
    "        command=['python', 'get_data.py'],\n",
    "        )\n",
    "    return training_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.component\n",
    "def data_preprocessing():    \n",
    "    # Defining component configuration\n",
    "    training_component = kfp.dsl.ContainerOp(\n",
    "        name='Data-Preparation',\n",
    "        image='docker.io/<docker_username>/kubeflow-sdk',\n",
    "        command=['python', 'process_data.py'],\n",
    "        )\n",
    "    return training_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.component\n",
    "def training():\n",
    "    # Defining component configuration\n",
    "    training_component = kfp.dsl.ContainerOp(\n",
    "        name='training',\n",
    "        image='docker.io/<docker_username>/kubeflow-sdk',\n",
    "        command=['python', 'train.py'],\n",
    "        file_outputs={'mlpipeline-ui-metadata':'/mlpipeline-ui-metadata.json', \"mlpipeline-metrics\":'/mlpipeline-metrics.json'}\n",
    "        )\n",
    "    \n",
    "    return training_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let see output of component configuration\n",
    "debug = False \n",
    "if debug :\n",
    "    training_component_vis = tensorflow_object_detection_api_training_component()\n",
    "    print(training_component_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Kubeflow pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it will define how component will run in the flow\n",
    "@kfp.dsl.pipeline(\n",
    "  name=\"tablemodel.csv file analysis\",\n",
    "  description=\"predicting cc or ssn\"\n",
    ")\n",
    "def dummy():\n",
    "    download_data = get_data()\n",
    "    download_data.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    data_processing = data_preprocessing().after(download_data)\n",
    "    data_processing.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    train = training().after(data_processing)\n",
    "    train.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let see output of pipeline configuration\n",
    "debug = False \n",
    "if debug :\n",
    "    training_pipeline_output = dummy()\n",
    "    print(training_component_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Kubeflow Pipeline \n",
    "* It will Generate .zip file inside this contain YAMl file which contain the configuration of kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(dummy, 'kubeflow-sdk-demo.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to deployed kubeflow pipeline Endpoint (GCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create kfp client\n",
    "# # Note: Add the KubeFlow Pipeline endpoint below if the client is not running on the same cluster.\n",
    "client = kfp.Client(\"<endpoint-address>")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'dummy-1'\n",
    "experiment = client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy pipeline to kubeflow pipeline Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(experiment.id, 'dummy-1', 'kubeflow-sdk-demo.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
